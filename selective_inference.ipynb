{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Inference\n",
    "\n",
    "The Models are evaluated by blocking certain codebook vectors, with high misclassification rates and testing if an improvement ind adversarial accuracy is achieved.\n",
    "\n",
    "This is achieved by setting their weights to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic S-VQVAE import\n",
    "def get_s_vqvae_model_class(model_path):\n",
    "    # extract codebook vector size from filename\n",
    "    match = re.search(r\"emb(\\d+)\", model_path)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Could not find codebook vector size in filename: {model_path}\")\n",
    "    emb_size = match.group(1)\n",
    "\n",
    "    # map the size to the appropriate module and class import\n",
    "    model_imports = {\n",
    "        \"512\": \"models.s_vqvae_emb512\",\n",
    "        \"256\": \"models.s_vqvae_emb256\",\n",
    "        \"128\": \"models.s_vqvae_emb128\",\n",
    "        \"64\":  \"models.s_vqvae_emb64\",\n",
    "        \"10\":  \"models.s_vqvae_emb10\"\n",
    "    }\n",
    "\n",
    "    if emb_size not in model_imports:\n",
    "        raise ValueError(f\"Unsupported codebook vector size: {emb_size}\")\n",
    "\n",
    "    module_name = model_imports[emb_size]\n",
    "    module = __import__(module_name, fromlist=['S_VQVAE'])\n",
    "    return module.S_VQVAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other model options\n",
    "from models.pretrainedvgg16_256 import S_VQVAE_VGG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loader\n",
    "def load_model(model_path, device):\n",
    "\n",
    "\n",
    "    # basic models without variations\n",
    "    model_classes = {\n",
    "        \"cnn_classifier\": CNNClassifier(num_classes=10),\n",
    "        \"vae\": VAE(),\n",
    "    }\n",
    "\n",
    "    # attempt to dynamically import an S_VQVAE class\n",
    "    try:\n",
    "        s_vqvae_class = get_s_vqvae_model_class(model_path)\n",
    "        model_classes[\"s_vqvae\"] = s_vqvae_class()\n",
    "    except ValueError as ve:\n",
    "        print(f\"S_VQVAE Import Error: {ve}\")\n",
    "\n",
    "    if \"s_vqvae_vgg\" in model_path.lower():\n",
    "        model_classes[\"s_vqvae_vgg\"] = S_VQVAE_VGG()\n",
    "    elif \"vgg16\" in model_path.lower():\n",
    "        pass\n",
    "\n",
    "    # load checkpoint from file\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"File not found: {model_path}\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load checkpoint from {model_path}: {e}\")\n",
    "\n",
    "    for model_type, model in model_classes.items():\n",
    "        try:\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model_state_dict = checkpoint['state_dict']\n",
    "            else:\n",
    "                model_state_dict = checkpoint\n",
    "\n",
    "            model_keys = set(model.state_dict().keys())\n",
    "            checkpoint_keys = set(model_state_dict.keys())\n",
    "\n",
    "            # check for key mismatch\n",
    "            if not model_keys.issubset(checkpoint_keys):\n",
    "                print(f\"Key mismatch for {model_type}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # load\n",
    "            model.load_state_dict(model_state_dict, strict=False)\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            print(f\"Successfully loaded {model_path} as {model_type}\")\n",
    "            return model, model_type\n",
    "\n",
    "        except RuntimeError as re:\n",
    "            print(f\"RuntimeError for {model_type}: {re}\")\n",
    "        except KeyError as ke:\n",
    "            print(f\"KeyError for {model_type}: {ke}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for {model_type}: {e}\")\n",
    "\n",
    "    raise ValueError(f\"Model loading failed for {model_path}. No compatible architecture found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Failure Rates under PGD-k Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pgd_attack(model, images, labels, eps=8/255, alpha=2/255, iters=10):\n",
    "\n",
    "    device = images.device\n",
    "    adv_images = images.clone().detach()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        adv_images.requires_grad_()\n",
    "        _, _, logits = model(adv_images)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            adv_images = adv_images + alpha * adv_images.grad.sign()\n",
    "            perturbation = torch.clamp(adv_images - images, min=-eps, max=eps)\n",
    "            adv_images = torch.clamp(images + perturbation, min=0, max=1)\n",
    "\n",
    "    return adv_images.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate clean accuracy\n",
    "def evaluate_clean_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(model.quantize.embed.device), labels.to(model.quantize.embed.device)\n",
    "        dec, diff, logits = model(images)  \n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate PGD-k robustness\n",
    "def evaluate_pgd_robustness(model, dataloader, device, eps=8/255, alpha=2/255, iters=10):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy on PGD adversarial images for a single model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # No model_type; just pass the necessary arguments\n",
    "        adv_images = pgd_attack(model, images, labels, eps, alpha, iters)\n",
    "\n",
    "        _, _, logits = model(adv_images)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional flip rate computation\n",
    "def compute_flip_rate(model, dataloader, eps=8/255, alpha=2/255, iters=10):\n",
    "    \"\"\"\n",
    "    How often does the label flip from clean to adv predictions?\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    flip_count = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(model.quantize.embed.device), labels.to(model.quantize.embed.device)\n",
    "\n",
    "        # clean predictions\n",
    "        _, _, clean_logits = model(images)\n",
    "        clean_preds = clean_logits.argmax(dim=1)\n",
    "\n",
    "        # PGD adversarial\n",
    "        adv_images = pgd_attack(model, images, labels, eps, alpha, iters)\n",
    "        _, _, adv_logits = model(adv_images)\n",
    "        adv_preds = adv_logits.argmax(dim=1)\n",
    "\n",
    "        flip_count += (clean_preds != adv_preds).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return flip_count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_codebook_usage_and_misclassification_pgd(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    eps=8/255,\n",
    "    alpha=2/255,\n",
    "    iters=10\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    num_codebook_vectors = model.quantize.embed.shape[1]\n",
    "\n",
    "    usage_count = torch.zeros(num_codebook_vectors, dtype=torch.long, device=device)\n",
    "    miscount = torch.zeros(num_codebook_vectors, dtype=torch.long, device=device)\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # create adversarial images\n",
    "        adv_images = pgd_attack(model, images, labels, eps, alpha, iters)\n",
    "\n",
    "        # codebook indices\n",
    "        quant, diff, embed_ind = model.encode(adv_images)\n",
    "\n",
    "        logits = model.classify(quant)\n",
    "\n",
    "        # evaluate correctness\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct_mask = (preds == labels)\n",
    "\n",
    "        flat_indices = embed_ind.view(embed_ind.size(0), -1)\n",
    "\n",
    "        for i in range(flat_indices.size(0)):\n",
    "            used_indices = flat_indices[i]\n",
    "            usage_count.index_add_(\n",
    "                0,\n",
    "                used_indices,\n",
    "                torch.ones_like(used_indices, dtype=torch.long)\n",
    "            )\n",
    "            if not correct_mask[i]:\n",
    "                miscount.index_add_(\n",
    "                    0,\n",
    "                    used_indices,\n",
    "                    torch.ones_like(used_indices, dtype=torch.long)\n",
    "                )\n",
    "\n",
    "    return usage_count, miscount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_codebook_misclassification_rates_pgd(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    eps=8/255,\n",
    "    alpha=2/255,\n",
    "    iters=10\n",
    "):\n",
    "\n",
    "    usage_count, miscount = compute_codebook_usage_and_misclassification_pgd(\n",
    "        model, dataloader, device, eps, alpha, iters\n",
    "    )\n",
    "\n",
    "    usage_count = usage_count.float()\n",
    "    miscount = miscount.float()\n",
    "\n",
    "    mis_rates = torch.zeros_like(miscount)\n",
    "    nonzero_mask = (usage_count > 0)\n",
    "    mis_rates[nonzero_mask] = miscount[nonzero_mask] / usage_count[nonzero_mask]\n",
    "\n",
    "    return mis_rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking Process\n",
    "\n",
    "Blocking based on misclassification Threshold, to avoid failing codebook vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_codebook_vectors(model, high_error_indices):\n",
    "    with torch.no_grad():\n",
    "        # set weight of failing codebook vectors to zero\n",
    "        model.quantize.embed[:, high_error_indices] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_codebook_blocking_pgd(model, dataloader, device, eps, alpha, iters, thresholds):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # collect stats\n",
    "        total_clean = 0\n",
    "        total_pgd   = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # loop over data\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        results.append({\n",
    "            \"threshold\": threshold,\n",
    "            \"clean_acc\": total_clean / total_samples,\n",
    "            \"pgd_acc\": total_pgd / total_samples\n",
    "\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def evaluate_models_in_folder(\n",
    "    folder_path,\n",
    "    dataloader,\n",
    "    device,\n",
    "    thresholds=[1.00, 0.75, 0.50],\n",
    "    eps=8/255,\n",
    "    alpha=2/255,\n",
    "    iters=10\n",
    "):\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith(('.pt', '.pth'))]\n",
    "    model_files.sort()\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(folder_path, model_file)\n",
    "        print(f\"\\n=== Evaluating model: {model_file} ===\")\n",
    "\n",
    "        model, detected_type = load_model(model_path, device)\n",
    "\n",
    "        # evaluate single model\n",
    "        blocked_results_df = evaluate_with_codebook_blocking_pgd(\n",
    "            model=model,\n",
    "            dataloader=dataloader,\n",
    "            device=device,\n",
    "            eps=eps,\n",
    "            alpha=alpha,\n",
    "            iters=iters,\n",
    "            thresholds=thresholds\n",
    "        )\n",
    "\n",
    "        # annotate with model name, type\n",
    "        blocked_results_df[\"model_name\"] = model_file\n",
    "        blocked_results_df[\"model_type\"] = detected_type\n",
    "\n",
    "        # save\n",
    "        base_name, _ = os.path.splitext(model_file)\n",
    "        out_csv = f\"results/codebook_blocking_{base_name}.csv\"\n",
    "        blocked_results_df.to_csv(out_csv, index=False)\n",
    "        print(blocked_results_df)\n",
    "        print(f\"Saved results to: {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual model evaluation\n",
    "def evaluate_models_individually_in_folder(\n",
    "    folder_path,\n",
    "    dataloader,\n",
    "    device,\n",
    "    thresholds=[0.75, 0.90, 0.95, 0.98],\n",
    "    eps=8/255,\n",
    "    alpha=2/255,\n",
    "    iters=10\n",
    "):\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith(('.pt', '.pth'))]\n",
    "    model_files.sort()\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(folder_path, model_file)\n",
    "        print(f\"\\n=== Evaluating model: {model_file} ===\")\n",
    "\n",
    "\n",
    "        model, detected_type = load_model(model_path, device)\n",
    "\n",
    "        model = model.to(device, dtype=torch.float32)\n",
    "\n",
    "        blocked_results_df = evaluate_with_codebook_blocking_pgd(\n",
    "            model=model,\n",
    "            dataloader=dataloader,\n",
    "            device=device,\n",
    "            eps=eps,\n",
    "            alpha=alpha,\n",
    "            iters=iters,\n",
    "            thresholds=thresholds\n",
    "        )\n",
    "\n",
    "        blocked_results_df[\"model_name\"] = model_file\n",
    "        blocked_results_df[\"model_type\"] = detected_type\n",
    "\n",
    "        print(blocked_results_df)\n",
    "\n",
    "        # save each model's result to its own csv\n",
    "        base_name, _ = os.path.splitext(model_file)\n",
    "        out_csv = f\"results/codebook_blocking_{base_name}.csv\"\n",
    "        blocked_results_df.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved results to: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blocking process\n",
    "if __name__ == \"__main__\":\n",
    "    import torchvision.transforms as transforms\n",
    "    import torchvision.datasets as datasets\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    " \n",
    "    # CIFAR-10 test dataset and loader\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # directory of models\n",
    "    folder_path = \"best_model/blockage\"\n",
    "\n",
    "    # thresholds for blocking, 1.01 for no blocking\n",
    "    thresholds = [1.01, 1.00, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40]\n",
    "\n",
    "    # evaluate each model in folder_path\n",
    "    evaluate_models_individually_in_folder(\n",
    "        folder_path=folder_path,\n",
    "        dataloader=test_loader,\n",
    "        device=device,\n",
    "        thresholds=thresholds,\n",
    "        eps=8/255,\n",
    "        alpha=2/255,\n",
    "        iters=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's first calculate the relative changes of blocking certain thresholds and combine the individual results for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved changes: results\\codebook_blocking_svqvae_cd_delayedat_emb256_marginchange_lr05_changes.csv\n",
      "Saved changes: results\\codebook_blocking_svqvae_cd_delayedat_emb256_marginchange_lr05_changes_changes.csv\n",
      "Saved changes: results\\codebook_blocking_svqvae_emb256_changes.csv\n",
      "Saved changes: results\\codebook_blocking_svqvae_emb256_changes_changes.csv\n",
      "Saved changes: results\\codebook_blocking_svqvae_emb256_delayedat_changes.csv\n",
      "Saved changes: results\\codebook_blocking_svqvae_emb256_delayedat_changes_changes.csv\n",
      "Saved changes: results\\codebook_blocking_under_pgd_ALL_MODELS_changes.csv\n",
      "Saved changes: results\\codebook_blocking_under_pgd_ALL_MODELS_changes_changes.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "all_csvs = glob.glob(\"results/codebook_blocking_*.csv\")\n",
    "\n",
    "for csv_file in all_csvs:\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # baseline first value, so no blocking\n",
    "    baseline_clean_acc = df.loc[0, \"clean_acc\"]\n",
    "    baseline_pgd_acc   = df.loc[0, \"pgd_acc\"]\n",
    "    baseline_flip_rate = df.loc[0, \"flip_rate\"]\n",
    "\n",
    "    # compute relative changes\n",
    "    df[\"clean_acc_change\"] = (df[\"clean_acc\"] - baseline_clean_acc) / baseline_clean_acc * 100\n",
    "    df[\"pgd_acc_change\"]   = (df[\"pgd_acc\"]   - baseline_pgd_acc)   / baseline_pgd_acc   * 100\n",
    "    df[\"flip_rate_change\"] = (df[\"flip_rate\"] - baseline_flip_rate) / baseline_flip_rate * 100\n",
    "\n",
    "    # save a new CSV with suffix `_changes`\n",
    "    new_file = csv_file.replace(\".csv\", \"_changes.csv\")\n",
    "    df.to_csv(new_file, index=False)\n",
    "    print(f\"Saved changes: {new_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# plot function for changes\n",
    "def plot_changes_bar_chart_in_axis(df_changes, ax, figure_title=\"\"):\n",
    "    x_labels = df_changes[\"threshold\"].astype(str).values\n",
    "\n",
    "    # rename \"1.01\" -> \"no blockage\"\n",
    "    x_labels = [\"no blockage\" if lbl == \"1.01\" else lbl for lbl in x_labels]\n",
    "\n",
    "    clean_vals = df_changes[\"clean_acc_change\"].values\n",
    "    pgd_vals   = df_changes[\"pgd_acc_change\"].values\n",
    "    flip_vals  = df_changes[\"flip_rate_change\"].values\n",
    "\n",
    "    x = np.arange(len(x_labels))\n",
    "    width = 0.25\n",
    "\n",
    "    # grouped bars\n",
    "    ax.bar(x - width, clean_vals, width=width, label=\"Clean Acc %Δ\")\n",
    "    ax.bar(x,         pgd_vals,   width=width, label=\"PGD Acc %Δ\")\n",
    "    ax.bar(x + width, flip_vals,  width=width, label=\"Flip Rate %Δ\")\n",
    "\n",
    "    # horizontal line at 0\n",
    "    ax.axhline(y=0, color='grey', linewidth=1, linestyle='--')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Percentage Change (%)\")\n",
    "    ax.set_title(figure_title)\n",
    "    ax.legend()\n",
    "\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_three_models_individually(results_folder=\"results/collection\"):\n",
    "\n",
    "    # map file names for better visual clarity\n",
    "    csv_map = [\n",
    "        (\n",
    "            \"codebook_blocking_svqvae_cd_delayedat_emb256_marginchange_lr05_changes.csv\",\n",
    "            \"SVQVAE256 CD DelayedAT\"\n",
    "        ),\n",
    "        (\n",
    "            \"codebook_blocking_svqvae_emb256_changes.csv\",\n",
    "            \"SVQVAE256\"\n",
    "        ),\n",
    "        (\n",
    "            \"codebook_blocking_svqvae_emb256_delayedat_changes_changes.csv\",\n",
    "            \"SVQVAE256 DelayedAT\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    subplot_labels = [\"(a)\", \"(b)\", \"(c)\"]\n",
    "\n",
    "    # 3×1 subplots \n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(14, 10))\n",
    "    fig.suptitle(\"Relative Codebook Blocking Performance by Threshold\", fontsize=16)\n",
    "\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "    for i, (csv_file, display_title) in enumerate(csv_map):\n",
    "        csv_path = os.path.join(results_folder, csv_file)\n",
    "\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"File not found: {csv_path}. Skipping subplot {i}.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        ax = axes[i] \n",
    "\n",
    "        title_with_label = f\"{subplot_labels[i]} {display_title}\"\n",
    "\n",
    "        # plot bars\n",
    "        plot_changes_bar_chart_in_axis(df, ax, figure_title=title_with_label)\n",
    "\n",
    "    for j in range(i + 1, 3):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  \n",
    "    out_fig = os.path.join(results_folder, \"3models_changes_subplots.png\")\n",
    "    plt.savefig(out_fig, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved subplot figure with 3 models at: {out_fig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved subplot figure with 3 models at: results/collection\\3models_changes_subplots.png\n"
     ]
    }
   ],
   "source": [
    "visualize_three_models_individually(results_folder=\"results/collection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
